# Airflow

## 개발 가이드라인

[AIRFLOW_DEV_GUIDE](AIRFLOW_DEV_GUIDE.md) 참고

## ETL 파이프라인

매일 대상 차량과 관련된 커뮤니티 데이터를 수집하고 분석하여 적재합니다.
`etl_single_model.py` 파일을 참고

## 1. 크롤링

유튜브와 자동차 커뮤니티를 대상으로 게시글/댓글 데이터를 수집합니다.

### 장애 대응 전략

1. 크롤링 실패 시 실패한 URL을 별도로 저장한 뒤 재시도합니다.  
   일시적인 네트워크 문제로 인해 크롤링이 실패하는 경우가 있습니다. 이 경우 이 단계에서 복구됩니다.
2. 재시도 후에도 실패한 URL은 별도로 저장하고 개발자에게 알립니다.  
   이 경우는 HTML 구조 변경, API 변경 등 다양한 원인이 있습니다. 이 경우 개발자가 원인을 파악하고 조치하도록 합니다.

### Validation

크롤링이 끝나면 크롤링 결과(JSON)를 검증합니다. 검증은 크롤링 결과가 올바른 형식(필드 이름, 데이터 타입)인지 확인합니다. 이 시점의 데이터 검증은 다음과같은 목적을 가집니다:

- 휴먼 에러 방지
  - 개발 과정에서 필드 이름이나 타입이 잘못 지정되는 경우를 많이 겪습니다.
- 장애 감지 및 전파 방지
  - 웹사이트가 변경되었거나 API 스펙이 변경되는 경우 크롤링 결과가 유효하지 않을 수 있습니다. 이 경우 미리 앞단에서 장애를 감지하고 개발자에게 알립니다.

## 2. 데이터 처리

EMR 클러스터를 생성한 후 Step을 제출합니다. Spark를 통해 게시글과 댓글을 문장 단위로 분리하고 결측치를 제거합니다. 제거된 결측치는 추후 처리를 위해 별도로 저장합니다.

## 3. 데이터 분석

문장을 분석하여 카테고리, 키워드, 감성 등을 추출합니다. 분석에는 ChatGPT API를 사용하며 Lambda를 병렬로 호출하여 처리합니다.

## 4. 데이터 적재

수집한 데이터와 분석 결과를 Redshift에 적재합니다. 적재 전 데이터 검증을 수행하여 데이터 무결성을 보장합니다.
